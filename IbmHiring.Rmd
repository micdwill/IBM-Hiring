---
title: "IBM Acquisition Team Recommendations"
author: "Michael Williams, Connor Martin, Zafir Maker-Agha, Alan Mcmillin, Matthew Feigenbaum"
date: "March 18th"
output: 
  html_document:
    theme: cerulean
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Our Problem

## Our Audience

Our audience comprises the acquisition team of IBM (International Business Machines Corporation), a global technology conglomerate renowned for its diverse hiring practices and varied workforce productivity. Central to their concerns is the challenge of worker attrition. Our objective is to provide guidance that ensures the recruitment of high-quality candidates with a diminished likelihood of turnover. By optimizing hiring practices in this manner, we aim to enhance profitability in both the short and long term for IBM.

## Our Question

Our paramount concern is addressing how we can optimize both the caliber and overall productivity of our workforce, while safeguarding against premature turnover. At the core of this inquiry lies the imperative to maximize the profitability derived from each hire, considering that recruiting individuals who swiftly depart leads to negative returns on investment, particularly in terms of training costs. <br>

Hence, it is imperative that we meticulously screen candidates to avert scenarios where early attrition undermines our bottom line

## Our Narrative

IBM's workforce spans across various fields, yet recent trends reveal a concerning pattern of premature departures, resulting in significant profit setbacks. The root of the issue lies in the acquisition team's emphasis on short-term productivity, overlooking the crucial factor of employee retention. <br>

Similar to university admissions strategies, where institutions like the University of Michigan might forgo exceptional applicants due to concerns about their commitment, IBM must adopt a nuanced approach. We propose a strategy that balances maximizing immediate output with the long-term goal of retaining top talent. <br>

Our solution involves screening candidates not only for their potential contributions to IBM but also for their propensity to remain with the company. By prioritizing individuals who demonstrate both high potential and a commitment to long-term engagement, we mitigate the risk of negative profit margins associated with frequent turnover. <br>

In essence, our tailored models enable IBM to navigate the seemingly paradoxical challenge of selecting candidates who are both highly productive and likely to stay. By investing in employees who align with the company's long-term vision, IBM can secure greater profitability in the years ahead. <br>

## How we Quantify our Work

In order to quantify the impact of our work, we have assigned monetary values to our final outcomes, streamlining worker productivity into a comprehensible variable. This variable encompasses several factors: tenure, job level, engagement, overtime commitment, performance ratings, and compensation, each meticulously calibrated to accurately reflect worker quality. Furthermore, we've quantified the financial impact of attrition, ensuring clarity and ease of interpretation for our results. <br>

When our model is applied to test data, its efficacy can be scrutinized and effectively communicated. Transparency is paramount in our reporting. Additionally, we've excluded variables that can only be determined post-hire from our training data, facilitating its utility for future candidate evaluations by the IBM acquisition team. 

While our model serves as a valuable tool, it's important to acknowledge its limitations. It's not infallible and should be complemented with other considerations such as resume details and interview performance. Our recommendations are based on upfront data, like marital status or proximity to the office, and should be integrated with holistic hiring practices.

It's essential to note that our model may not always identify the most qualified candidates, as it prioritizes retention probability over immediate output. This strategic focus might initially impact short-term productivity. However, the long-term benefits of reduced attrition and sustained workforce stability outweigh these potential shortfalls.

# Data Exploration

## Read in Data

We read in the data set and take a quick look at it.

```{r}
employee <- read.csv("attrition.csv")
str(employee)
summary(employee)
```

## Graph Attrition

We make a simple bar plot of the attrition variable

```{r}
library(ggplot2)
ggplot(employee, aes(x = factor(Attrition), fill = factor(Attrition))) +
  geom_bar(color = "black") +
  labs(title = "                             Distibution of Attrition",
       x = "",
       y = "Amount") +
  scale_x_discrete(labels = c("Not Attrition", "Attrition")) +
  scale_fill_manual(values = c("darkgrey", "white"), guide = FALSE) +  # Set fill colors manually
  theme_minimal() +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        axis.line = element_line(color = "black"))
```

## Clean data

We delete some useless variables and factor those that need to be factored. We then take another look at our updated data.
```{r}
employee$Attrition <- as.factor(employee$Attrition)
employee$BusinessTravel <- as.factor(employee$BusinessTravel)
employee$Department <- as.factor(employee$Department)
employee$EducationField <- as.factor(employee$EducationField)
employee$Gender <- as.factor(employee$Gender)
employee$JobRole <- as.factor(employee$JobRole)
employee$MaritalStatus <- as.factor(employee$MaritalStatus)
employee$Over18 <- NULL
employee$OverTime <- as.factor(employee$OverTime)
str(employee)
summary(employee)
```

## View Data

We run a for loop to look at a histogram of all of our numerical variables. This gives us a better idea of the makeup of the data.

```{r}
library(ggplot2)

numerical_vars <- names(employee)[sapply(employee, is.numeric)]

for (var in numerical_vars) {
  plot_title <- paste("Histogram of", var)
  print(
    ggplot(employee, aes_string(x = var)) +
      geom_histogram(fill = "lightblue", color = "black") +
      labs(title = plot_title, x = var, y = "Frequency") +
      theme_minimal()
  )
}

```

We notice that some variables are useless as they are the same for every observation. Therefore, we delete these variables. We also notice that performance rating is actually a binary variable so we choose to factor it.

```{r}

employee$EmployeeCount <- NULL
employee$StandardHours <- NULL
employee$EmployeeNumber <- NULL
employee$PerformanceRating <- as.factor(employee$PerformanceRating)
str(employee)

```

We now take a look at a correlation matrix of all of our remaining numerical variables.

```{r}
numerical_vars <- employee[, sapply(employee, is.numeric)]

correlation_matrix <- cor(numerical_vars)

print(correlation_matrix)

```

We see the only numerical variables that are heavily correlated are monthly income and job level. They have an R^2 of about $0.95$. This makes sense and the correlation is not perfectly $1$. Additionally, both variables seem very relevant to our question, so we choose to keep them both.

## Make Variable to Predict

We decide that we want to predict on our data before any hire is made. This means that we have to delete variables that can only be determined by following a hire. This includes variables such as job satisfaction and years at company. <br> Before we do this however, we make a variable for prediction. We call it employee quality. "Quality" is representative of the amount of money an employee makes our company daily. This is why we are subtracting monthly income divided 30. Any data that is a positive indicator of job performance or output leads to a greater employee quality. Research led us to allocate the weight of each variable. For instance, increased job involvement from employees has been shown to lead to massive increases in a profit for companies. <br> Admittedly, this metric is quite arbitrary. It is most definitely not a perfect representation of profit per customer, as this is likely impossible to represent with a single number. This is because many factors can not be measured, and some things such as interaction between co workers can not be shown with one number. At any rate however, this provides us a benchmark that certainly has some significance. <br> After this is done, we look at a histogram of our new quality variable. Notice that it is mostly normally distributed, further ensuring its validity as an accurate measurement of quality. Also notice that the mean quality is $-16$ dollars, suggesting that we are likely underestimating profit per worker.

```{r}
employee$Quality <- with(employee, (100 * YearsAtCompany / Age) + (40 * JobInvolvement) + (20 * JobLevel) + ifelse(OverTime == "Yes", 30, 0) + ifelse(PerformanceRating == "4", 150, 0) - (MonthlyIncome / 30))

employee$YearsAtCompany <- NULL
employee$YearsInCurrentRole <- NULL
employee$JobInvolvement <- NULL
employee$JobLevel <- NULL
employee$OverTime <- NULL
employee$PerformanceRating <- NULL
employee$MonthlyIncome <- NULL
employee$DailyRate <- NULL
employee$MonthlyRate <- NULL
employee$HourlyRate <- NULL
employee$StockOptionLevel <- NULL
employee$YearsSinceLastPromotion <- NULL
employee$YearsWithCurrManager <- NULL
employee$PercentSalaryHike <- NULL
employee$TrainingTimesLastYear <- NULL
employee$JobSatisfaction <- NULL
str(employee)
summary(employee)
# Assuming employee$Quality contains your quality data
hist(employee$Quality, 
     main = "Histogram of Employee Quality", 
     col = 'black', 
     border = 'white',
     xlab = "$ Added per Day",
     breaks = seq(-500, 350, by = 50))  # Adjust the breaks argument to set intervals of 50 on the x-axis

mean(employee$Quality)

```

## Normalize Data

We have to normalize our data in order to run some of our models. This ensures that all of our data has equal weight. Otherwise, some variables would be have too much weight in determining our predictions.

```{r}
employeedummy <- as.data.frame(model.matrix(~. -1, data=employee))

normalize <- function(x){
  (x - min(x))/(max(x) - min(x))
}

employee_n <- as.data.frame(lapply(employeedummy, normalize))

```

## Prepare for Quality Prediction

We make Quality a binary variable for prediction. We decide that if a worker has an output of over $50$ dollars a day, they are a quality worker. Once again, this is an arbitrary cutoff, but it gives us something to work with. It also ensures that even if our quality metric is prone to error, those who are rated as "high quality" are still very likely to produce a profit. <br>
We then look at a graph showing the main problem with IBM's hires. The quality workers are those who are most likely to quit. Despite quality workers making up about $25$% of workers, about $50$% of those who quit are among those quality workers. This is the fundamental issue with our current hires. We are hiring for quality, but many of them are quitting. We need a more holistic hire process, where we also search for loyal workers.

```{r}
employee_n$Quality <- employee$Quality
employee_n$Quality <- ifelse(employee_n$Quality > 50, 1, 0)

library(ggplot2)
ggplot(employee_n, aes(x = factor(AttritionYes), fill = factor(Quality))) +
  geom_bar(position = "stack", color = "black") +
  labs(title = "                                Attrition vs Quality",
       x = "",
       y = "Count") +
  scale_x_discrete(labels = c("Not Attrition", "Attrition", "Not Attrition, Attrition")) +
  scale_fill_manual(values = c("white", "black"), guide = FALSE) +
  facet_wrap(~factor(Quality, labels = c("Not Quality", "Quality"))) +
  theme_minimal() +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        axis.line = element_line(color = "black"),
        strip.background = element_blank(),
        strip.text.x = element_text(size = 12, face = "bold"))
```

## Prepare for Attrition Prediction

We make a different very similar data set to predict attrition. We are careful not to include quality when prediction attrition, and vice versa. We are now prepared to make models predicting both attrition and quality. We take a quick at our datasets to confirm.

```{r}
attrition <- employee_n
attrition$Quality <- NULL
attrition$AttritionNo <- NULL
employee_n$AttritionNo <- NULL
employee_n$AttritionYes <- NULL
str(employee_n)
summary(employee_n)
summary(attrition)
```

## Test/Train

We create a test/train split for our data. We will build our models with the train data and evaluate our models with the test data. We choose a ratio of 0.5 to ensure that we have ample data for both testing and training. We split our attrition and quality data sets with the same splits. Additionally, we split our numerical variable with the same split for later evaluation.

```{r}
ratio <- 0.5
set.seed(122121)
trainRows <- sample(1:nrow(employee_n), ratio*nrow(employee_n))

employeeTrain <- employee_n[trainRows, ]
employeeTest <- employee_n[-trainRows, ]

employeeTestLabel <- employeeTest$Quality
employeeTrainLabel <- employeeTrain$Quality

employeeTestPredictors <- employeeTest[,-29]
employeeTrainPredictors <- employeeTrain[,-29]

attritionTrain <- attrition[trainRows, ]
attritionTest <- attrition[-trainRows, ]

attritionTestLabel <- attritionTest$AttritonYes
attritionTrainLabel <- attritionTrain$AttritionYes

attritionTestPredictors <- attritionTest[,-2]
attritionTrainPredictors <- attritionTrain[,-2]

quality <- employee[-trainRows,]

```

# Predicitng Quality

## Make GLM Model

We use our train data to build a GLM model for quality. Once again, this only includes variables that can be acquired prior to a hire. We then check to see how our model predicts on the test data. We have a Kappa of $0.40$.

```{r}
library(caret)
GlmModel <- glm(Quality~., data=employeeTrain, family="binomial")
summary(GlmModel)
glmPred <- predict(GlmModel, newdata=employeeTest, type = "response")
glmBin <- ifelse(glmPred >= 0.5, 1, 0)
confusionMatrix(as.factor(glmBin), as.factor(employeeTest$Quality), positive = "1")
```

## Make KNN Model

We build our KNN model for employee quality and save our predictions. KNN takes a test data point and finds the data points from the train data that our closest to our test data point. It then uses a majority vote to choose what we assign this variable as (1 or 0). "K" is the amount of variables close to our test data point from the train data that we use to predict. We try many $k$'s and we determined that a $k$ of $13$ works the best on the data. Our rule of thumb says it should be $\sqrt{735} \approx 27$ but this gives us a much worse model. This is likely due to the low amount of variables. We then evaluate how our model performs on the test data. We see that we have a Kappa of $0.40$.

```{r}
library(class)
KnnModel <- knn(train = employeeTrainPredictors, test = employeeTestPredictors, cl = employeeTrainLabel, k = 13)
confusionMatrix(as.factor(KnnModel), as.factor(employeeTest$Quality), positive = "1")
```

## Make ANN Model

We make our Neural Network model to predict quality. This model is based on the human brain. In each layer, every neuron is connected to every neuron in the previous layer. We use $5$ hidden layers of $60$, $30$, $10$, $6$, and $4$ for a total of $110$ neurons. We increase our learning rate factor and threshold in order to ensure that our program runs in a reasonable time. We then make predictions on our test data. We will not save our binary predictions because we will allow our decision tree to find a good threshold. However, we will use them to evaluate our model. We see that we achieve a Kappa of $0.43$.

```{r}
library(neuralnet)
set.seed(422)


annmodel <- neuralnet(Quality ~ ., data = employeeTrain, hidden = c(60, 30, 10,6,4), threshold = 5,
  stepmax = 1e+05, rep = 1, startweights = NULL,
  learningrate.limit = NULL, learningrate.factor = list(minus = 0.5,
  plus = 1.2), learningrate = NULL, lifesign = "none",
  lifesign.step = 1000, algorithm = "rprop+", err.fct = "sse",
  act.fct = "logistic", linear.output = TRUE, exclude = NULL,
  constant.weights = NULL, likelihood = FALSE)
```

```{r}
library(caret)
annPred <- predict(annmodel, employeeTest)
annBin <- ifelse(annPred >= 0.5, 1, 0)
confusionMatrix(as.factor(annBin), as.factor(employeeTest$Quality), positive = "1")
```

## Make SVM Model
We make our SVM model and make binary predictions on quality. We try many different kernels and evaluate the kappa of each model on the test data. We save the model with the best kappa to be our SVM model that goes into our decision tree. After running our model, we see that Vanilla performs the best so we save that model. It has a Kappa of $0.48$.


```{r}
library(kernlab)
library(caret)

kernels <- c("vanilladot", "rbfdot", "polydot", "tanhdot", "laplacedot", "besseldot", "anovadot", "splinedot")
best_kappa <- -Inf
best_model <- NULL
best_predictions <- NULL

for (kernel in kernels) {
  classifier <- ksvm(factor(Quality) ~ ., data = employeeTrain, kernel = kernel)
  predictions <- predict(classifier, employeeTest)
  predictions <- as.factor(predictions)
  cm <- confusionMatrix(as.factor(predictions), as.factor(employeeTest$Quality), positive = "1")
  kappa_value <- cm$overall["Kappa"]
  
  if (kappa_value > best_kappa) {
    best_kappa <- kappa_value
    best_model <- kernel
    best_predictions <- predictions
  }
}

# Save the predictions of the best model to a dataframe
svmPred <- data.frame(Predictions = as.character(best_predictions))
svmPredictions <- as.factor(svmPred$Predictions)

# Print the best model and its kappa
cat("Best Model:", best_model, "- Best Kappa:", best_kappa, "\n")


```

## Decision Tree Model

Now we make our basic decision tree model for quality. Many branches are made with binary decisions for many different variables. At each leaf, $1$ or $0$ is chosen. We will feed these predictions into our larger decision tree model. First, we will evaluate these predictions. We see that we have a Kappa of $0.39$.

```{r}
library(C50)
dt <- C5.0(as.factor(Quality) ~., data = employeeTrain)
plot(dt)

dtpredict <- predict(dt, employeeTest)
confusionMatrix(as.factor(dtpredict), as.factor(employeeTest$Quality), positive = "1")
```

## Making a Data Frame of All of Our Models

We combine all of our previous model's predictions into a single data frame for our employee quality predictions. We are also sure to put our quality prediction into the data set so that we are able to train our model. If a non-binary prediction exists we are sure to enter that one into the data frame as want our final decision tree to make the thresholds for us. We will build our final model later.

```{r}
employeeModels <- data.frame(dtpredict, annPred, svmPredictions, KnnModel, glmPred, employeeTest$Quality)
str(employeeModels)
```



# Prediciting Attrition


## Make GLM Model

We now use our train data to make a GLM model for prediction attrition. We have a Kappa of $0.23$, indicating early on that our attrition model is less powerful than our quality model.

```{r}
library(caret)
GlmModel <- glm(AttritionYes~., data=attritionTrain, family="binomial")
summary(GlmModel)
glmPred <- predict(GlmModel, newdata=attritionTest, type = "response")
glmBin <- ifelse(glmPred >= 0.5, 1, 0)
confusionMatrix(as.factor(glmBin), as.factor(attritionTest$AttritionYes), positive = "1")
```

## Make KNN Model

We build our KNN model for attrition. This time, we find that a $k$ of $6$ works best. Once again, our model performs worse than it did for quality with a Kappa of $0.19$.

```{r}
library(class)
set.seed(8)
KnnModel <- knn(train = attritionTrainPredictors, test = attritionTestPredictors, cl = attritionTrainLabel, k = 6)
confusionMatrix(as.factor(KnnModel), as.factor(attritionTest$AttritionYes), positive = "1")
```

## Make ANN Model

We make a our Neural Network model for predicting attrition. We once again use $5$ hidden layers of $60$, $30$, $10$, $6$, and $4$ for a total of $110$ neurons. We have a final Kappa of $0.23$, once again worse than the quality prediction.

```{r}
library(neuralnet)
set.seed(422)


annmodel <- neuralnet(AttritionYes ~ ., data = attritionTrain, hidden = c(60, 30, 10,6,4), threshold = 2,
  stepmax = 1e+05, rep = 1, startweights = NULL,
  learningrate.limit = NULL, learningrate.factor = list(minus = 0.5,
  plus = 1.2), learningrate = NULL, lifesign = "none",
  lifesign.step = 1000, algorithm = "rprop+", err.fct = "sse",
  act.fct = "logistic", linear.output = TRUE, exclude = NULL,
  constant.weights = NULL, likelihood = FALSE)
```

```{r}
library(caret)
annPred <- predict(annmodel, attritionTest)
annBin <- ifelse(annPred >= 0.5, 1, 0)
confusionMatrix(as.factor(annBin), as.factor(attritionTest$AttritionYes), positive = "1")
```

## Make SVM Model
We now make an SVM model for attrition. We once again try many different kernels, with Anova performing the best this time. It has a Kappa of $0.20$, once again much lower than our Kappa for quality.


```{r}
library(kernlab)
library(caret)

kernels <- c("vanilladot", "rbfdot", "polydot", "tanhdot", "laplacedot", "besseldot", "anovadot", "splinedot")
best_kappa <- -Inf
best_model <- NULL
best_predictions <- NULL

for (kernel in kernels) {
  classifier <- ksvm(factor(AttritionYes) ~ ., data = attritionTrain, kernel = kernel)
  predictions <- predict(classifier, attritionTest)
  predictions <- as.factor(predictions)
  cm <- confusionMatrix(as.factor(predictions), as.factor(attritionTest$AttritionYes), positive = "1")
  kappa_value <- cm$overall["Kappa"]
  
  if (kappa_value > best_kappa) {
    best_kappa <- kappa_value
    best_model <- kernel
    best_predictions <- predictions
  }
}

# Save the predictions of the best model to a dataframe
svmPred <- data.frame(Predictions = as.character(best_predictions))
svmPredictions <- as.factor(svmPred$Predictions)

# Print the best model and its kappa
cat("Best Model:", best_model, "- Best Kappa:", best_kappa, "\n")


```

## Decision Tree Model

Now we make our basic decision tree model for attrition. We see that we have a Kappa of $0.20$, per usual this signifies less predicting power than our quality model.

```{r}
library(C50)
dt <- C5.0(as.factor(AttritionYes) ~., data = attritionTrain)
plot(dt)

dtpredict <- predict(dt, attritionTest)
confusionMatrix(as.factor(dtpredict), as.factor(attritionTest$AttritionYes), positive = "1")
```

## Making a Data Frame of All of Our Models

We combine all of our previous model's predictions into a single data frame for attrition predictions. We are also sure to put our response variable, attrtion, into the data set so that we are able to train our model. If a non-binary prediction exists we are sure to enter that one into the data frame as want our final decision tree to make the thresholds for us.

```{r}
attritionModels <- data.frame(dtpredict, annPred, svmPredictions, KnnModel, glmPred, attritionTest$AttritionYes)
str(attritionModels)
```

# Making Final Model

## Break Data Frame into Test/Train

We now break our final data frame into test and train with a ratio $.7/.3$. This will allow us to train our stacked decision trees and test on them. We split our data for both attrition and quality. Additionally, we give the same split to our numerial quality variable for later evaluation.

```{r}
ratio <- 0.7
set.seed(69)
trainRowsFinal <- sample(1:nrow(employeeModels), ratio*nrow(employeeModels))
employeeTrain <- employeeModels[trainRowsFinal, ]
employeeTest <- employeeModels[-trainRowsFinal, ]

attritionTrain <- attritionModels[trainRowsFinal,]
attritionTest <- attritionModels[-trainRowsFinal,]

quality <- quality[-trainRowsFinal,]
quality <- quality$Quality
```

## Quality Final Tree with Cost Matrix

Now we make our stacked decision tree for quality. We use different costs for false positives and false negatives. We associate a cost of $1.$ for false negatives and $1.25$ for false positives. The standard setting for a decision tree assigns a cost of $1$ for both. In this way, we are looking to avoid false positives as we want to ensure that we are hiring quality workers. 

```{r}
#1.25, 1
cost_matrix <- matrix(c(0,1.25,1,0), nrow = 2) 
finalDt <- C5.0(as.factor(employeeTest.Quality) ~., data = employeeTrain, costs = cost_matrix)
plot(finalDt)

employeepredict <- predict(finalDt, employeeTest)
confusionMatrix(as.factor(employeepredict), as.factor(employeeTest$employeeTest.Quality), positive = "1")

```

Our stacked model performed better than any of the base models, with a Kappa of $0.49$. Also, our cost matrix led to a less amount of false positives, meaning we are mostly selecting quality workers.

## Attrition Final Tree with Cost Matrix

It is paramount that we do not hire workers who are will just quit. Therefore we assign a cost of $5$ for false negatives and only $1$ for false positives. This will ensure that almost everyone we predict will not quit, will not actually quit.

```{r}
#1, 5
cost_matrix <- matrix(c(0,1,5,0), nrow = 2) 
finalDt <- C5.0(as.factor(attritionTest.AttritionYes) ~., data = attritionTrain, costs = cost_matrix)
plot(finalDt)

attritionpredict <- predict(finalDt, attritionTest)
confusionMatrix(as.factor(attritionpredict), as.factor(attritionTest$attritionTest.AttritionYes), positive = "1")

```

We have a kappa of about $0.39$, which is significantly better than all of our base models. This shows the power of the cost matrix. Additionally, we have an extremely small number of false negatives, as intended. This shows that our model has performed very well for our goal.

## Plot all of our Kappas

We make a plot to look at the performance of all our models. This allows us to better understand our performance. Notice the massive improvement in our attrition Kappa with our ultimate, stacked model.

```{r}
library(ggplot2)
library(tidyr)

# Example data
Quality <- c(0.40, 0.40, 0.43, 0.48, 0.39, 0.49)
Attrition <- c(0.23, 0.19, 0.23, 0.20, 0.20, 0.39)
Models <- c("GLM", "KNN", "ANN", "SVM", "DT", "Ultimate")
data <- data.frame(Models, Quality, Attrition)

# Reshape the data
melted_data <- data %>%
  pivot_longer(cols = c(Quality, Attrition), names_to = "Variable", values_to = "Kappa")

# Plot
ggplot(melted_data, aes(x = Models, y = Kappa, fill = Variable)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8), color = "darkgrey") +
  scale_fill_manual(values = c("black", "white")) +  # Set colors manually
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

## Make Final Predictions

We decide to hire someone if we predict that they are both a quality worker, and they will not quit. We call this "doHire." We make confusion matrices to see how many of hires are quality and how many will eventually quit. We see that we chose to hire only $4$ workers who will eventually quit, less than $15%$ of our hires. <br>

We also check how accurate our model is on choosing to hire. We check the actual quality and attrition values and then use the same criteria for hire. We then compare these results to our predictions. We did quite well, with $18$ hires being good hires and only $15$ being poor. With all of the variables we had deleted, this is a staggering performance. We have a final Kappa of $0.32$.


```{r}
doHire <- ifelse(employeepredict == 1 & attritionpredict == 0, 1, 0)

conf <- confusionMatrix(as.factor(doHire), as.factor(attritionTest$attritionTest.AttritionYes), positive = "1")

confusionMatrix(as.factor(doHire), as.factor(employeeTest$employeeTest.Quality), positive = "1")

shouldHire <-ifelse(employeeTest$employeeTest.Quality == 1 & attritionTest$attritionTest.AttritionYes == 0, 1, 0)

conf_matrix <- confusionMatrix(as.factor(doHire), as.factor(shouldHire), positive = "1")

print(conf_matrix)

conf_matrix_df <- as.data.frame(as.matrix(conf_matrix$table))

conf_df <- as.data.frame(as.matrix(conf$table))


ggplot(data = conf_matrix_df, aes(x = Reference, y = Prediction)) +
  geom_tile(aes(fill = Freq), colour = "black") +  # Change the color of borders to black
  geom_text(aes(label = Freq), color = "black", family = "Arial") +  # Change text color to black and font family to Arial
  theme_minimal() +
  scale_fill_gradient(low = "white", high = "darkgrey") +
  labs(x = "Ideal Hire Choice", y = "Model's Hire Choice", title = "                       Confusion Matrix of Hires") +
  theme(axis.text = element_text(size = 12, family = "Times New Roman"),  # Change font family for axis text
        axis.title = element_text(size = 14, face = "bold", family = "Times New Roman"),  # Change font family for axis titles
        plot.title = element_text(size = 16, face = "bold", family = "Times New Roman"))


ggplot(data = conf_df, aes(x = Reference, y = Prediction)) +
  geom_tile(aes(fill = Freq), colour = "black") +  # Change the color of borders to black
  geom_text(aes(label = Freq), color = "black", family = "Arial") +  # Change text color to black and font family to Arial
  theme_minimal() +
  scale_fill_gradient(low = "white", high = "darkgrey") +
  labs(x = "Hire would Quit", y = "Recommended Hire", title = "         Recommended Hire Compared to Attrition") +
  theme(axis.text = element_text(size = 12, family = "Times New Roman"),  # Change font family for axis text
        axis.title = element_text(size = 14, face = "bold", family = "Times New Roman"),  # Change font family for axis titles
        plot.title = element_text(size = 16, face = "bold", family = "Times New Roman"))
```

## Finding our Final Profit

We kept our numerical quality from earlier for a reason. We want to evaluate our total final profit. First, we look a histogram of the Quality of our recommended hires. Notice that even those below 50, which we rate as "Not Quality," are still above $0$ or not far below it. This is a very good sign. Finally, we take our $4$ workers who will quit out of our data set and we add up the remaining hires quality. This gives us our final increase in dollars per hour. We have a net profit of about $2,500$ dollars. This means we have $$\frac{2,500}{33} \approx 75 \text{ dollars per hire}$$

```{r}
qualityOfHires = data.frame(quality, doHire, attritionTest$attritionTest.AttritionYes) 

subset <- qualityOfHires[qualityOfHires$doHire == 1,]

hist(subset$quality, 
     main = "Employee Quality of Recommended Hires", 
     col = 'black', 
     border = 'white',
     xlab = "$ Added per Day",
     breaks = seq(-50, 250, by = 25))

subset <- subset[subset$attritionTest.attritionTest.AttritionYes == 0,]

sum(subset$quality)
```

# Conclusions

Our model was largely successful and powerful on predicting who we should hire based on our metrics. This is made evident by the average profit per day per worker of $75$ dollars. This number may be based on our arbitrary assumptions, but it is such as massive increase from the initial $-16$ dollars that its power can not be denied. And at any rate, we are increasing variables that are certainly correlated with increased job performance. Its power is meaningful as we initially deleted any variables highly correlated with on the variables we were predicting. The only variables remaining were those giving basic information on a candidate such as education and distance from home. This ensures that our model can be used when trying to make hires.<br>

With the power of the cost matrix, we were able to nearly entirely avoid workers who will eventually quit. This would lead to much more long-term profits. Our recommended hires are of both high quality and likely to be loyal. This is what led to about $75$ extra dollars per day per worker with our recommended hires. <br>

With that said, our model was not perfect. Our model did recommend hiring some individuals who would not lead to increased profits. Due to this, we recommend that our model be used in conjunction with other hiring methods. This may include typical resume drops or interviews. In this way, with a holistic approach, IBM can make the most profitable hires possible. <br>

Expanding on that previous note, our current model is based on the idea that we are using the model as the sole method for hiring. The results are good, but they can be improved upon with a holistic approach. This entails using our model to screen candidates for interviews. Therefore, we would entirely change the cost matrices. We currently have $(0, 5, 4, 0)$ for quality. If we were screening, we may change this to $(0,1,1,0)$. Likewise, we may change our attrition cost matrix from $(0,1,5,0)$ to $(0,1,2,0)$.